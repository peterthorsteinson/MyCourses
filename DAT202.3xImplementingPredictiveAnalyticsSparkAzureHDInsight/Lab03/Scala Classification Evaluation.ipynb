{"nbformat_minor": 1, "cells": [{"source": "## Evaluating a Classification Model\n\nIn this exercise, you will create a pipeline for a classification model, and then apply commonly used metrics to evaluate the resulting classifier.\n\n### Prepare the Data\n\nFirst, import the libraries you will need and prepare the training and test data:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Import Spark SQL and Spark ML libraries\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.classification.LogisticRegression\n\n// Load the source data\nval csv = spark.read.option(\"inferSchema\",\"true\").option(\"header\", \"true\").csv(\"wasb:///data/flights.csv\")\n\n// Select features and label\nval data = csv.select($\"DayofMonth\", $\"DayOfWeek\", $\"OriginAirportID\", $\"DestAirportID\", $\"DepDelay\", ($\"ArrDelay\" > 15).cast(\"Int\").alias(\"label\"))\n\n// Split the data\nval splits = data.randomSplit(Array(0.7, 0.3))\nval train = splits(0)\nval test = splits(1).withColumnRenamed(\"label\", \"trueLabel\")", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Define the Pipeline and Train the Model\nNow define a pipeline that creates a feature vector and trains a classification model", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Define the pipeline\nval assembler = new VectorAssembler().setInputCols(Array(\"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\")).setOutputCol(\"features\")\nval lr = new LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\").setMaxIter(10).setRegParam(0.3)\nval pipeline = new Pipeline().setStages(Array(assembler, lr))\n\n// Train the model\nval model = pipeline.fit(train)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Test the Model\nNow you're ready to apply the model to the test data.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val prediction = model.transform(test)\nval predicted = prediction.select(\"features\", \"prediction\", \"trueLabel\")\npredicted.show(100, truncate=false)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Compute Confusion Matrix Metrics\nClassifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n- True Positives\n- True Negatives\n- False Positives\n- False Negatives\n\nFrom these core measures, other evaluation metrics such as *precision* and *recall* can be calculated.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val tp = predicted.filter(\"prediction == 1 AND truelabel == 1\").count().toFloat\nval fp = predicted.filter(\"prediction == 1 AND truelabel == 0\").count().toFloat\nval tn = predicted.filter(\"prediction == 0 AND truelabel == 0\").count().toFloat\nval fn = predicted.filter(\"prediction == 0 AND truelabel == 1\").count().toFloat\nval metrics = spark.createDataFrame(Seq(\n (\"TP\", tp),\n (\"FP\", fp),\n (\"TN\", tn),\n (\"FN\", fn),\n (\"Precision\", tp / (tp + fp)),\n (\"Recall\", tp / (tp + fn)))).toDF(\"metric\", \"value\")\nmetrics.show()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### View the Raw Prediction and Probability\nThe prediction is based on a raw prediction score that describes a labelled point in a logistic function. This raw prediction is then converted to a predicted label of 0 or 1 based on a probability vector that indicates the confidence for each possible label value (in this case, 0 and 1). The value with the highest confidence is selected as the prediction.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "prediction.select(\"rawPrediction\", \"probability\", \"prediction\", \"trueLabel\").show(100, truncate=false)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Note that the results include rows where the probability for 0 (the first value in the **probability** vector) is only slightly higher than the probability for 1 (the second value in the **probability** vector). The default *discrimination threshold* (the boundary that decides whether a probability is predicted as a 1 or a 0) is set to 0.5; so the prediction with the highest probability is always used, no matter how close to the threshold.", "cell_type": "markdown", "metadata": {}}, {"source": "### Review the Area Under ROC\nAnother way to assess the performance of a classification model is to measure the area under a ROC curve for the model. the spark.ml library includes a **BinaryClassificationEvaluator** class that you can use to compute this. The ROC curve shows the True Positive and False Positive rates plotted for varying thresholds.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n\nval evaluator = new BinaryClassificationEvaluator().setLabelCol(\"trueLabel\").setRawPredictionCol(\"rawPrediction\").setMetricName(\"areaUnderROC\")\nval auc = evaluator.evaluate(prediction)\nprintln(\"AUC = \" + (auc))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Change the Discrimination Threshold\nThe AUC score seems to indicate a reasonably good model, but the performance metrics seem to indicate that it predicts a high number of False Negative labels (i.e. it predicts 0 when the true label is 1), leading to a low Recall. You can affect the way a model performs by changing its parameters. For example, as noted previously, the default discrimination threshold is set to 0.5 - so if there are a lot of False Positives, you may want to consider raising this; or conversely, you may want to address a large number of False Negatives by lowering the threshold.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Redefine the pipeline\nval lr2 = new LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\").setThreshold(0.35).setMaxIter(10).setRegParam(0.3)\nval pipeline2 = new Pipeline().setStages(Array(assembler, lr2))\n\n// Retrain the model\nval model2 = pipeline2.fit(train)\n\n// Retest the model\nval newPrediction = model2.transform(test)\nnewPrediction.select(\"rawPrediction\", \"probability\", \"prediction\", \"trueLabel\").show(100, truncate=false)\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Note that some of the **rawPrediction** and **probability** values that were previously predicted as 0 are now predicted as 1", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Recalculate confusion matrix\nval tp2 = newPrediction.filter(\"prediction == 1 AND truelabel == 1\").count().toFloat\nval fp2 = newPrediction.filter(\"prediction == 1 AND truelabel == 0\").count().toFloat\nval tn2 = newPrediction.filter(\"prediction == 0 AND truelabel == 0\").count().toFloat\nval fn2 = newPrediction.filter(\"prediction == 0 AND truelabel == 1\").count().toFloat\nval metrics2 = spark.createDataFrame(Seq(\n (\"TP\", tp2),\n (\"FP\", fp2),\n (\"TN\", tn2),\n (\"FN\", fn2),\n (\"Precision\", tp2 / (tp2 + fp2)),\n (\"Recall\", tp2 / (tp2 + fn2)))).toDF(\"metric\", \"value\")\nmetrics2.show()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Note that there are now more True Positives and less False Negatives, and Recall has improved. By changing the discrimination threshold, the model now gets more predictions correct - though it's worth noting that the number of False Positives has also increased.", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}